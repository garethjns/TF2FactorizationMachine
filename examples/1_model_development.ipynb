{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Factorization machine in TensorFlow 2\n",
    "\n",
    "Example implementing a factorization machine in TensorFlow 2 using eager computation.\n",
    "\n",
    "To oversimplify, factorization machines are like linear/logistic regression models, but with an added set of weights, *V*, that models polynomial interactions in a lower dimensional space. They're particualry suited to very sparse inputs, such as user-item ratings for reccomendation systems.\n",
    "\n",
    "This is based on the theory and tf-1 code in [this article](http://nowave.it/factorization-machines-with-tensorflow.html) by Gabriele Modena and the paper by [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) by Steffen Rendle. Please refer to these articles for much more detailed descriptions of the model, and the maths that avoid O(p<sup>2</sup>).\n",
    "\n",
    "Note this is early version of the model so may well contain mistakes. Also to avoid having to deal with TFs typing, everything is cast into double precision for now."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "2.0.0-beta1\n",
      "True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Callable, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Toy data example\n",
    "\n",
    "This data is modified slightly from:\n",
    "http://nowave.it/factorization-machines-with-tensorflow.html\n",
    "http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "\n",
    "It contains user and item information required for collaborative filtering (and also some additional features). One hot encoded where appropriate. Ratings are explicit 1-5.\n",
    "\n",
    "| Users | Movies | Movie ratings | Time | Last movies rated | Rating |\n",
    "|---|---|---|---|---|--|\n",
    "| **[A, B, C]** | **[TI, NH, SW, ST]** | **[TI, NH, SW, ST]** | **[T]** | **[TI, NH, SW, ST]** | **[R]** | \n",
    "| [1, 0, 0] | [1,  0,  0,  0] | [0.3, 0.3, 0.3, 0.0] |  13 |  [0,  0,  0,  0 ] | 5 | \n",
    "| [1, 0, 0] | [0,  1,  0,  0] | [0.3, 0.3, 0.3, 0.0] |  14 |  [1,  0,  0,  0 ] | 3 |\n",
    "| [1, 0, 0] | [0,  0,  1,  0] | [0.3, 0.3, 0.3, 0.0] |  16 |  [0,  1,  0,  0 ] | 1 |\n",
    "| [0, 1, 0] | [0,  0,  1,  0] | [0.0, 0.0, 0.5, 0.5] |  5  |  [0,  0,  0,  0 ] | 4 |\n",
    "| [0, 1, 0] | [0,  0,  0,  1] | [0.0, 0.0, 0.5, 0.5] |  8  |  [0,  0,  1,  0 ] | 5 |\n",
    "| [0, 0, 1] | [1,  0,  0,  0] | [0.5, 0.0, 0.5, 0.0] |  9  |  [0,  0,  0,  0 ] | 1 |\n",
    "| [0, 0, 1] | [0,  0,  1,  0] | [0.5, 0.0, 0.5, 0.0] |  12 |  [1,  0,  0,  0 ] | 5 |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(7, 16)\n(7, 1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Features\n",
    "x = np.array([[1, 0, 0,  1, 0, 0, 0,  0.3, 0.3, 0.3, 0.0,  13,  0, 0, 0, 0 ],\n",
    "              [1, 0, 0,  0, 1, 0, 0,  0.3, 0.3, 0.3, 0.0,  14,  1, 0, 0, 0 ],\n",
    "              [1, 0, 0,  0, 0, 1, 0,  0.3, 0.3, 0.3, 0.0,  16,  0, 1, 0, 0 ],\n",
    "              [0, 1, 0,  0, 0, 1, 0,  0.0, 0.0, 0.5, 0.5,  5,   0, 0, 0, 0 ],\n",
    "              [0, 1, 0,  0, 0, 0, 1,  0.0, 0.0, 0.5, 0.5,  8,   0, 0, 1, 0 ],\n",
    "              [0, 0, 1,  1, 0, 0, 0,  0.5, 0.0, 0.5, 0.0,  9,   0, 0, 0, 0 ],\n",
    "              [0, 0, 1,  0, 0, 1, 0,  0.5, 0.0, 0.5, 0.0,  12,  1, 0, 0, 0 ]])\n",
    "\n",
    "# Targets (explicit rating)\n",
    "y = np.array([5, 3, 1, 4, 5, 1, 5])\n",
    "y.shape = (7, 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class FactorizationMachine:\n",
    "    def __init__(self, \n",
    "                 m: int=2,\n",
    "                 k: int=5):\n",
    "        \"\"\"\n",
    "        Initialise coefficients. \n",
    "        \n",
    "        For now:\n",
    "         - Requires manual specification of feature space dimensionality. \n",
    "         - Coefficients are all float64. Might be overkill but it simplifies casting.\n",
    "        \n",
    "        :param m: Number of features.\n",
    "        :param k: Number of latent factors to model in V.\n",
    "        \"\"\"\n",
    "        self.b = tf.Variable(tf.zeros([1], \n",
    "                                      dtype='double'))\n",
    "        self.w = tf.Variable(tf.random.normal(([m]), \n",
    "                                              stddev=0.01, \n",
    "                                              dtype='double'))\n",
    "        self.v = tf.Variable(tf.random.normal(([k, m]), \n",
    "                                              stddev=0.01, \n",
    "                                              dtype='double'))\n",
    "        \n",
    "    \n",
    "    def __call__(self, x: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Predict from model.\n",
    "        \n",
    "        :param x: Tensor containing features. \n",
    "        :return: Tensor containing predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Linear terms\n",
    "        linear = tf.reduce_sum(tf.multiply(self.w, x), \n",
    "                                           axis=1, \n",
    "                                           keepdims=True)\n",
    "\n",
    "        # Interaction terms\n",
    "        interactions = tf.multiply(0.5, tf.reduce_sum(tf.subtract(tf.pow(tf.matmul(x, tf.transpose(self.v)), 2),\n",
    "                                                      tf.matmul(tf.pow(x, 2), tf.transpose(tf.pow(self.v, 2)))),\n",
    "                                                      axis=1, \n",
    "                                                      keepdims=True))\n",
    "\n",
    "        # Linear sum along with intercept\n",
    "        wv = tf.add(linear, interactions)\n",
    "        bwv = tf.add(self.b, wv)\n",
    "        \n",
    "        return bwv\n",
    "    \n",
    "    \n",
    "mod = FactorizationMachine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define loss and training step\n",
    "\n",
    "The loss here is just least square error.\n",
    "\n",
    "The training step calculates the current model loss and automatically calculates and records the gradients using tf.GradientTape. The model coefficients are updated using these gradients and the learning rate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def l2_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def train_step(mod, x, y_true, lr,\n",
    "               loss_f: Callable=l2_loss):\n",
    "   \n",
    "    # Calculate current loss and record gradients\n",
    "    with tf.GradientTape() as t:\n",
    "        cur_loss = loss_f(y_true=y_true,\n",
    "                          y_pred=mod(x))\n",
    "        \n",
    "    # Get the gradients and assign to model\n",
    "    db, dw, dv = t.gradient(cur_loss, [mod.b, mod.w, mod.v])\n",
    "    mod.b.assign_sub(lr * db)\n",
    "    mod.w.assign_sub(lr * dw)\n",
    "    mod.v.assign_sub(lr * dv)\n",
    "\n",
    "    return cur_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model\n",
    "\n",
    "Training is done by iterating over the toy data a few times; there's no optimizer used here. Loss and the first elements of W and V are logged on each iteration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-902bdd408d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                      \u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                      \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0025\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                      loss_f=l2_loss)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6dbdcdabe384>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(mod, x, y_true, lr, loss_f)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         cur_loss = loss_f(y_true=y_true,\n\u001b[1;32m---> 10\u001b[1;33m                           y_pred=mod(x))\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Get the gradients and assign to model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-519fad9553b7>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Interaction terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         interactions = tf.multiply(0.5, tf.reduce_sum(tf.subtract(tf.pow(tf.matmul(x, tf.transpose(self.v)), 2),\n\u001b[0m\u001b[0;32m     40\u001b[0m                                                       tf.matmul(tf.pow(x, 2), tf.transpose(tf.pow(self.v, 2)))),\n\u001b[0;32m     41\u001b[0m                                                       \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2646\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2647\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6283\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6284\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6285\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6286\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6287\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\envs\\TF2FactorizationMachine\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(7, 16), b.shape=(16, 5), m=7, n=5, k=16 [Op:MatMul] name: MatMul/"
     ],
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(7, 16), b.shape=(16, 5), m=7, n=5, k=16 [Op:MatMul] name: MatMul/",
     "output_type": "error"
    }
   ],
   "source": [
    "# Reset model\n",
    "mod = FactorizationMachine(m=16)\n",
    "\n",
    "epochs = 200\n",
    "bs, ws, vs, losses = [], [], [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    cur_loss = train_step(mod=mod, \n",
    "                     x=x, \n",
    "                     y_true=y,\n",
    "                     lr=0.0025,\n",
    "                     loss_f=l2_loss)\n",
    "    \n",
    "    # Logging\n",
    "    losses.append(cur_loss)\n",
    "    bs.append(mod.b.numpy()[0])\n",
    "    # Just logging changes in first terms here\n",
    "    ws.append(mod.w.numpy()[0])\n",
    "    vs.append(mod.v.numpy()[0])\n",
    "    \n",
    "    # Sometimes plot loss\n",
    "    if (e > 1) & (e % 10 == 0):\n",
    "        print(f\"Epoch {e + 1} / {epochs}: Loss: {cur_loss.numpy()}, last change: {np.abs(losses[-1] - losses[-2])}\")\n",
    "        \n",
    "    # Early stop?\n",
    "    if e > 10:\n",
    "        if np.mean(losses[-10:-1]) < 0.1:\n",
    "            print(f\"Early stopping, loss={cur_loss.numpy()}\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(bs, \n",
    "         label='b')\n",
    "plt.plot(ws,\n",
    "         label='w[0]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Coefficients history')\n",
    "\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mod.v.shape)\n",
    "plt.imshow(mod.v.numpy())\n",
    "plt.ylabel('Latent features')\n",
    "plt.xlabel('Item index')\n",
    "plt.title('V')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "fms",
   "language": "python",
   "display_name": "FMs"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}